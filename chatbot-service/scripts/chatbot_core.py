import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from huggingface_hub import snapshot_download

class VetChatbotCore:
    def __init__(self, base_model_id="Qwen/Qwen2.5-7B-Instruct", adapter_path="models/Qwen2.5-7B/7B-LoRA", chroma_db_dir="models/chroma_db"):
        self.base_model_id = base_model_id
        self.adapter_path = adapter_path
        self.chroma_db_dir = chroma_db_dir
        
        self.tokenizer = None
        self.model = None
        self.retriever = None
        
        self._initialize()

    def _download_hf_assets(self, hf_token):
        """HuggingFace Private ë¦¬í¬ì§€í† ë¦¬ì—ì„œ chroma_dbì™€ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë¡œì»¬ models/ í´ë”ë¡œ ë‹¤ìš´ë¡œë“œ"""
        repo_id = "20-team-daeng-ddang-ai/vet-chat"
        local_dir = "models"
        
        needs_download = False
        if not os.path.exists(self.chroma_db_dir):
            print(f"âš ï¸ Chroma DB not found at {self.chroma_db_dir}.")
            needs_download = True
        if self.adapter_path and not os.path.exists(self.adapter_path):
            print(f"âš ï¸ LoRA Adapter not found at {self.adapter_path}.")
            needs_download = True
            
        if needs_download:
            print(f"ğŸ“¥ Downloading required assets from {repo_id} into '{local_dir}/'...")
            try:
                snapshot_download(
                    repo_id=repo_id,
                    allow_patterns=["chroma_db/*", "Qwen2.5-7B/7B-LoRA/*"],
                    local_dir=local_dir,
                    token=hf_token
                )
                print("âœ… Download complete.")
            except Exception as e:
                print(f"âŒ Failed to download assets: {e}")

    def _initialize(self):
        hf_token = os.getenv("HUGGING_FACE_HUB_TOKEN")
        
        # ëª¨ë¸ êµ¬ë™ ì „ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë¡œì»¬ íŒŒì¼(models/)ë¡œ ê°•ì œ ë‹¤ìš´ë¡œë“œ
        self._download_hf_assets(hf_token)

        print(f"Loading tokenizer & models... (Base: {self.base_model_id}, Adapter: {self.adapter_path})")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_id, 
            token=hf_token
        )
        
        # Load Base Model
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_id,
            device_map="auto",
            torch_dtype=torch.bfloat16, 
            token=hf_token
        )
        
        # Inject LoRA Adapter
        # ë§Œì•½ ë¡œì»¬ í´ë”ì´ê±°ë‚˜ '/' ë¬¸ìê°€ í¬í•¨ëœ í—ˆê¹…í˜ì´ìŠ¤ ë ˆí¬ì§€í† ë¦¬ ì£¼ì†Œë¼ë©´ ì–´ëŒ‘í„° ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
        if self.adapter_path and (os.path.exists(self.adapter_path) or "/" in self.adapter_path):
            try:
                self.model = PeftModel.from_pretrained(
                    base_model, 
                    self.adapter_path,
                    token=hf_token
                )
                print("âœ… LoRA Adapter loaded successfully.")
            except Exception as e:
                print(f"âš ï¸ Failed to load LoRA Adapter '{self.adapter_path}': {e}. Running with Base Model only.")
                self.model = base_model
        else:
            self.model = base_model
            print("âš ï¸ No valid LoRA Adapter path provided. Running with Base Model only.")

        print("Loading Vector DB...")
        embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
        vectorstore = Chroma(
            persist_directory=self.chroma_db_dir, 
            embedding_function=embeddings,
            collection_name="vet_qa_collection"
        )
        # return_source_documents is automatically handled when we access metadata if we retrieve directly.
        self.retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
        print("âœ… Core initialization complete.")

    def generate_answer(self, message: str, user_context: dict, history: list):
        """
        Generate a response based on RAG context, user history, and dog profile.
        
        Args:
            message (str): Current user question
            user_context (dict): {"dog_age_years": 5, "dog_weight_kg": 4.5, "breed": "Maltese"}
            history (list): [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
        
        Returns:
            dict: { "answer": str, "citations": list of dicts }
        """
        # 1. RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
        docs = self.retriever.invoke(message)
        
        context_text = ""
        citations = []
        for idx, doc in enumerate(docs):
            context_text += f"[ê·¼ê±° ìë£Œ {idx+1}]\n{doc.page_content}\n\n"
            citations.append({
                "doc_id": doc.metadata.get("id", f"doc_{idx}"),
                "title": doc.metadata.get("title", "ìˆ˜ì§‘ëœ ìˆ˜ì˜í•™ ì§€ì‹"),
                "score": 1.0, # Chroma DB ë˜í¼ ê¸°ë³¸ê°’ì—ì„œëŠ” ì ìˆ˜ ìƒëµë¨. í•„ìš”ì‹œ ìœ ì‚¬ë„ ì¿¼ë¦¬ë¡œ ë³€ê²½ ê°€ëŠ¥
                "snippet": doc.page_content[:100] + "..."
            })

        # 2. ê°•ì•„ì§€ í”„ë¡œí•„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„±
        profile_str = ""
        if user_context:
            age = user_context.get("dog_age_years", "ì•Œìˆ˜ì—†ìŒ")
            weight = user_context.get("dog_weight_kg", "ì•Œìˆ˜ì—†ìŒ")
            breed = user_context.get("breed", "ì•Œìˆ˜ì—†ìŒ")
            profile_str = f"- ê²¬ì¢…: {breed}\n- ë‚˜ì´: {age}ì‚´\n- ì²´ì¤‘: {weight}kg\n"
        
        # 3. ê³ ë„í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
        # ë‹µë³€ì˜ í†¤ì•¤ë§¤ë„ˆ, í™˜ê° ë°©ì§€, ìœ ì € ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì§€ì‹œí•©ë‹ˆë‹¤.
        system_prompt = (
            "ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì „ë¬¸ì ì¸ ìˆ˜ì˜í•™ AI ì±—ë´‡ì…ë‹ˆë‹¤.\n"
            "ì•„ë˜ ì œê³µëœ [í™˜ì ì •ë³´]ì™€ [ì°¸ê³  ë¬¸ì„œ]ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
            "ì˜í•™ì  íŒë‹¨ì´ í•„ìš”í•˜ê±°ë‚˜ ìƒëª…ì´ ìœ„ê¸‰í•œ ìƒí™©ì´ë¼ë©´, ë°˜ë“œì‹œ 'ê·¼ì²˜ ë™ë¬¼ë³‘ì›ì— ë‚´ì›í•˜ì‹œë¼'ëŠ” ê¶Œê³ ë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
            "ë‹µë³€ì€ í•µì‹¬ì„ ì§šì–´ ê°„ê²°í•˜ê³  ì¹œì ˆí•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”."
        )

        user_prompt = f"[í™˜ì ì •ë³´]\n{profile_str}\n[ì°¸ê³  ë¬¸ì„œ]\n{context_text}\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{message}"

        # 4. History ê¸°ë°˜ ëŒ€í™” í…œí”Œë¦¿ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ìµœëŒ€ 4ê°œ(ìµœê·¼ 2ë²ˆì˜ í„´)ì˜ ì´ì „ ëŒ€í™”ë§Œ í¬í•¨í•˜ì—¬ ë¬¸ë§¥ ìœ ì§€ ë° VRAM ì ˆì•½
        for past_msg in history[-4:]:
            messages.append(past_msg)
            
        messages.append({"role": "user", "content": user_prompt})
        
        # 5. ëª¨ë¸ ì¶”ë¡ 
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                pad_token_id=self.tokenizer.eos_token_id,
                temperature=0.1,  # RAG í™˜ê²½ í™˜ê° ì–µì œ
                top_p=0.9
            )
        
        input_length = inputs.input_ids.shape[1]
        response_text = self.tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()
        
        return {
            "answer": response_text,
            "citations": citations
        }

# Example usage (for testing)
if __name__ == "__main__":
    chatbot = VetChatbotCore(
        base_model_id="Qwen/Qwen2.5-7B-Instruct",
        adapter_path="../lora-qwen-7b-final", # local path for testing
        chroma_db_dir="../chroma_db"        # local path for testing
    )
    
    test_context = {"dog_age_years": 8, "dog_weight_kg": 3, "breed": "í¬ë©”ë¼ë‹ˆì•ˆ"}
    test_history = [{"role": "user", "content": "ê°•ì•„ì§€ ì˜ˆë°©ì ‘ì¢…ì€ ì–¸ì œ ë§ì¶°ì•¼ í•´?"}, {"role": "assistant", "content": "ë§¤ë…„ ë§ì¶”ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."}]
    test_msg = "ìš°ë¦¬ ì• ê¸°ê°€ ì–´ì œë¶€í„° ë…¸ë€ í† ë¥¼ í•´. ë‚˜ì´ê°€ ë§ì•„ì„œ ê±±ì •ì¸ë° í•˜ë£¨ êµ¶ê¸¸ê¹Œ?"
    
    res = chatbot.generate_answer(test_msg, test_context, test_history)
    print("\n[ë‹µë³€]:\n", res["answer"])
    print("\n[ì¸ìš©ëœ ë¬¸ì„œ]:\n", res["citations"])
