# ğŸ¶ Vet Chatbot Service Guide

This document defines the architecture, expected API schemas, and core model logic for the `chatbot-service` (veterinary Q&A AI). It guides how to set up the LLM inference server and how it integrates with the `ai-orchestrator`.

## 1. Architecture Overview (Proposed)

To ensure low latency and isolated resource usage, the `chatbot-service` should run as a standalone FastAPI server processing heavy LLM generation tasks (RAG + 7B-LoRA model). The `ai-orchestrator` handles user authentication and routes NLP requests here.

### Recommended Folder Structure
```bash
/chatbot-service
â”œâ”€â”€ run.py                 # Application entry point (uvicorn)
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # FastAPI app & API Routes
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py      # App configuration (paths to models, DB)
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ chat_schema.py # Pydantic models for API request/response
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ chat_service.py # Service layer invoking the core script
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ chatbot_core.py    # [IMPORTANT] Core logic script (provided)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lora-qwen-7b-final # LoRA Adapter weights directory
â”‚   â””â”€â”€ chroma_db          # Vector DB directory for RAG
â””â”€â”€ requirements.txt
```

### Integration Flow
1. **User Input** -> The user sends a text/image message containing their dog's profile history.
2. `ai-orchestrator` routes the `chat` request via HTTP POST to `chatbot-service`. 
3. **`chatbot-service`** (using `chatbot_core.py`):
   - Performs RAG search retrieving top Context Documents.
   - Embeds dog profile context (age, weight, breed) into the System Prompt.
   - Generates the AI answer.
4. `ai-orchestrator` formats the final response and returns it to the client.

---

## 2. API Response Schema Specification

The `chatbot-service` must adhere to the following schema contract when receiving/sending payloads from/to the `ai-orchestrator`.

### Request (`POST /api/vet/chat`)
The payload includes conversation history, current message, and vital context like age/breed to allow personalized responses.

```json
{
  "dog_id": 105,
  "conversation_id": "c_uuid_string",
  "message": "ìš°ë¦¬ ì• ê¸°ê°€ ì–´ì ¯ë°¤ë¶€í„° ë…¸ë€ í† ë¥¼ í•˜ê³  ë°¥ì„ ì•ˆ ë¨¹ì–´. í•˜ë£¨ êµ¶ê¸¸ê¹Œ?",
  "image_url": null,
  "user_context": {
    "dog_age_years": 8,
    "dog_weight_kg": 4.5,
    "breed": "Maltese"
  },
  "history": [
    { "role": "user", "content": "ìœ ì‚°ê· ì„ ë¨¹ì´ëŠ”ê²Œ ì¢‹ì„ê¹Œ?" },
    { "role": "assistant", "content": "ë„¤ ë„ì›€ì´ ë©ë‹ˆë‹¤." }
  ]
}
```

### Response
Returns the generated string, time of generation, and explicit citations derived from the Vector DB search.

```json
{
  "dog_id": 105,
  "conversation_id": "c_uuid_string",
  "answered_at": "2024-05-15T09:30:00Z",
  "answer": "ë…¸ë€ìƒ‰ êµ¬í† ëŠ” ë‹´ì¦™ì„± êµ¬í† ë¡œ ìœ„ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì†Œí™”ê¸° ì´ìƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ 8ì‚´ ë…¸ë ¹ê²¬ ë§í‹°ì¦ˆì˜ ê²½ìš°, ì·Œì¥ì—¼ ë“± í•©ë³‘ì¦ì˜ ìœ„í—˜ì´ ìˆìœ¼ë¯€ë¡œ ì¦‰ì‹œ ê·¼ì²˜ ë™ë¬¼ë³‘ì›ì— ë‚´ì›í•˜ì‹œê³ , ì„ì˜ë¡œ í•˜ë£¨ë¥¼ êµ¶ê¸°ì§€ ë§ˆì„¸ìš”.",
  "citations": [
    {
      "doc_id": "vet_guide_041",
      "title": "ë…¸ë ¹ê²¬ ì†Œí™”ê¸° ì§ˆí™˜ ëŒ€ì²˜ ê°€ì´ë“œ",
      "score": 1.0,
      "snippet": "ë…¸ë€ìƒ‰ ê±°í’ˆ êµ¬í† ëŠ” ê³µë³µì´ë‚˜ ì†Œí™”ë¶ˆëŸ‰..."
    }
  ]	
}
```

---

## 3. The Core Script (`scripts/chatbot_core.py`)

We provide a specialized Python class `VetChatbotCore` in `chatbot_core.py`.
Service developers simply need to initialize this class at startup to hold the models in memory, and call its `generate_answer()` method per request.

### Usage Example Details
```python
from scripts.chatbot_core import VetChatbotCore

# Call ONCE during FastAPI startup
chatbot_engine = VetChatbotCore(
    base_model_id="Qwen/Qwen2.5-7B-Instruct",
    adapter_path="models/lora-qwen-7b-final",
    chroma_db_dir="models/chroma_db"
)

# On API Request
result = chatbot_engine.generate_answer(
    message=request.message,
    user_context=request.user_context,
    history=request.history
)

answer_string = result["answer"]
citations_list = result["citations"]
```

### Provided Capabilities:
1. **Low VRAM Base**: BFloat16 precision mapping fits inside 12GB+ GPUs comfortably.
2. **Context-Aware Prompting**: Automatically fuses `user_context` (breed, age) into the `system` instruction string to enhance personalization and prevent hallucinations.
3. **Chat Templates**: Restricts LLM from entering a 'repetition loop' by strictly adhering to `<|im_start|>` sequences.

---

## 4. Required Artifacts & Setup (Hugging Face & Vector DB)
Before running the service, you must prepare the LLM weights and the Vector DB.

### A. LLM Weights (Hugging Face)
- **Base Model**: `Qwen/Qwen2.5-7B-Instruct` (Auto-downloaded by Hugging Face `transformers` upon first run).
- **LoRA Adapter**: The fine-tuned weights for veterinary chat must be downloaded from our Hugging Face repository and placed in the `/models` directory.
  - **Repo URL**: `huggingface.co/20-team-daeng-ddang-ai/vet-chat`
  - **Path in Repo**: `Qwen2.5-7B/7B-LoRA` (ablation study í´ë” íŠ¸ë¦¬ êµ¬ì¡°)
  - You can use Git LFS or Python's `huggingface_hub` (`hf_hub_download`) to download this specific folder into `/models/lora-qwen-7b-final`.

### B. Vector DB (RAG)
The Vector DB (`chroma_db`) contains the pre-embedded veterinary knowledge base. 
**Crucially, this Vector DB must be stored in the `chatbot-service` (the GPU server), NOT the `ai-orchestrator`.**
- **Size & Management**: It is approximately ~265MB. Because this is slightly large for standard Git tracking (which often fails or bloats with binary files >100MB), **it is highly recommended to manage the Vector DB on Hugging Face alongside the model weights.**
- **Repo URL**: `huggingface.co/20-team-daeng-ddang-ai/vet-chat`
- **Path in Repo**: `chroma_db` (or whichever folder you uploaded it to)
- **Action**: Download the `chroma_db` folder from Hugging Face and place it in the `models/` directory of the `chatbot-service`.
